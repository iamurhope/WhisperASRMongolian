# Монгол хэл дээрх Whisper ASR Загвар

Монгол хэлний автомат ярианы таних (ASR) системд зориулсан Whisper архитектурын тусгай хувилбар. Common Voice датасет дээр сургасан.

**Хамгийн сайн загвар:** 23-р epoch, **15.33% WER** тестийн багц дээр

### Сургалтын Явц

Загвар бүх epoch-уудад тогтмол сайжирч байна:
- **Epoch 12-15:** WER 30.64%-аас 21.15% хүртэл буурсан
- **Epoch 16-19:** WER 21.09%-аас 18.23% хүртэл сайжирсан
- **Epoch 20-24:** WER хамгийн сайн үр дүн **15.33%** хүрсэн

## Загварын Архитектур

### Тохиргоо
```python
n_mels = 80                # Mel спектрограмын шинж чанаруудын тоо
n_ctx = 1500               # Контекстийн урт
n_heads = 8                # Attention толгойны тоо
n_audio_layers = 4         # Аудио энкодерын давхаргын тоо
n_text_layers = 4          # Текст декодерын давхаргын тоо
n_embed = 512              # Embedding хэмжээ
vocab_size = 51865         # Whisper олон хэлний толь
sample_rate = 16000        # Дээжлэлтийн хурд
dropout = 0.2              # Dropout хувь
```

### Онцлог Шинжүүд
- **Multi-head Self-Attention:** Аудио шинж чанарын нарийн харилцан хамаарлыг олж тогтоох 8 толгойтой attention механизм
- **Cross-Attention:** Энкодер ба декодер хооронд мэдээлэл солилцох боломжийг хангана
- **Label Smoothing:** Overfitting-ээс сэргийлж, зөв таамаглал хийх чадварыг нэмэгдүүлнэ
- **Learning Rate Scheduler:** ReduceLROnPlateau, үгийн алдааны хувь (WER) дээр үндэслэн learning rate-г бууруулна
- **Mixed Precision Training:** FP16 сургалт, GPU санах ойг хэмнэх ба сургалтын хурдыг нэмэгдүүлнэ

## Сургалтын Сайжруулалт

### Epoch 17-аас хойш хэрэгжүүлсэн

1. **Аудио Өгөгдлийн Өргөтгөл (Augmentation)**
   - Gaussian дуу чимээ нэмэх (σ=0.005)
   - Random gain/volume тохируулга (0.8-1.2)
   - Зөвхөн сургалтын багц дээр хэрэглэсэн

2. **Label Smoothing Loss**
   - Smoothing factor: 0.1
   - Хэт итгэлтэй таамаглалаас сергийлнэ

3. **Сургалтын Хурд Менежмент**
   - Анхны сургалтын хурд: 1e-5
   - Learning rate scheduler: ReduceLROnPlateau
   - Patience: 3 epochs
   - Хамгийн бага LR: 1e-7

4. **Өөрчлөгдсөн Гиперпараметрүүд**
   - Dropout: 0.1 → 0.2
   - Gradient clipping: 1.0 → 0.5
   - Early stopping patience: 5 → 8 epochs

## Датасет

### Common Voice Corpus 21.0 MN
```
Нийт өгөгдөл: ~29,300 сургалтын жишээ
├── Сургалт (80%): ~23,440 дуу
├── Шалгалт (10%): ~2,930 дуу
└── Тест (10%): ~2,930 дуу
```

### Өгөгдлийн боловсруулалт
- Аудио формат: MP3 → WAV
- Дээжлэлтийн хурд: 16kHz
- Mel-спектрограм: 80 давтамжийн бүс
- Хамгийн их урт: 1500 frame (~30 секунд)

## Жишээ таамаглалууд

### Epoch 24 (Хамгийн сайн загвар)

**Жишээ 1:**
- **Жинхэнэ:** Тиймээс сургаалыг сонсож байхдаа түүнд итгэх хэрэгтэй.
- **Таамаглал:** Хиймээс сургаалыг сонсож байхдаа түүнд итгэх хэрэгтэй.

**Жишээ 2:**
- **Жинхэнэ:** Энэ чинь итали нэр биш болохоор тогтооход хэцүү юм.
- **Таамаглал:** Энэ чинь машали нэр биш болохоор тогтооход хэцүү юм.

**Жишээ 3:**
- **Жинхэнэ:** Өөрөөр хэлбэл нэг хүүхдийнх биш хоёр хүүхдийн сургалтын төлбөр ажээ.
- **Таамаглал:** Өөрөөр хэлбэл нэг хүүхдийнх биш хоёр хүүхдийн сургалтын төлбөр ажээ. ✓

**Жишээ 4:**
- **Жинхэнэ:** Миний улцан цагаан Берлин орсон ч юм уу, хэн мэдэх вэ.
- **Таамаглал:** Миний улцан цагаан Берлин орсон ч юм уу, хэн мэдэх вэ. ✓

## Техникийн Дэлгэрэнгүй

### Оновчлогч (Optimizer)
- **AdamW** optimizer
- Learning rate: 1e-5
- Weight decay: 0.01
- Betas: (0.9, 0.999)

### Сургалтын Стратеги
- Batch size: 4
- Gradient accumulation: 4 (үр дүнгийн batch size = 16)
- Gradient clipping: max_norm=0.5
- Mixed precision training (FP16)

### Алдааны Функц
- Label Smoothing Cross Entropy
- Smoothing parameter: 0.1
- Ignore index: -100

## Загварын Үр Дүн

### Сүүлийн Үр Дүн (Сүүлийн 3 Epoch)

| Epoch | Сургалтын Алдаа | Шалгалтын Алдаа | Шалгалтын WER | Тестийн WER |
|-------|-----------------|-----------------|---------------|-------------|
| 22    | 1.5423          | 0.1552          | 0.1687        | **0.1640**  |
| 23    | 1.5338          | 0.1525          | 0.1608        | **0.1589**  |
| 24    | 1.5260          | 0.1477          | 0.1558        | **0.1533**  |
