{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzK0pMUHzbzk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import WhisperTokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class WhisperConfig:\n",
        "    def __init__(self,\n",
        "                 n_mels=80,\n",
        "                 n_ctx=1500,\n",
        "                 n_heads=8,\n",
        "                 n_audio_layers=4,\n",
        "                 n_text_layers=4,\n",
        "                 n_embed=512,\n",
        "                 n_audio_ctx=1500,\n",
        "                vocab_size=51865,\n",
        "                sample_rate=16000,\n",
        "                 dropout=0.1):\n",
        "        self.n_mels = n_mels\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_heads = n_heads\n",
        "        self.n_audio_layers = n_audio_layers\n",
        "        self.n_text_layers = n_text_layers\n",
        "        self.n_embed = n_embed\n",
        "        self.n_audio_ctx = n_audio_ctx\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sample_rate = sample_rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_embed = config.n_embed\n",
        "        assert self.n_embed % self.n_heads == 0\n",
        "\n",
        "        self.head_dim = self.n_embed // self.n_heads\n",
        "        self.wq = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.wk = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.wv = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.wo = nn.Linear(config.n_embed, config.n_embed)\n",
        "\n",
        "    def forward(self, x, mask=None, kv=None):\n",
        "        b, t, c = x.size()\n",
        "        q = self.wq(x).view(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if kv is None:\n",
        "            k = self.wk(x).view(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "            v = self.wv(x).view(b, t, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        else:\n",
        "            k = self.wk(kv).view(b, kv.size(1), self.n_heads, self.head_dim).transpose(1, 2)\n",
        "            v = self.wv(kv).view(b, kv.size(1), self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
        "\n",
        "        if mask is not None:\n",
        "            att = att.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        out = att @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, c)\n",
        "        return self.wo(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config.n_embed, 4 * config.n_embed)\n",
        "        self.w2 = nn.Linear(4 * config.n_embed, config.n_embed)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.w2(F.gelu(self.w1(x))))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.mask_attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "        self.cross_attn = MultiHeadAttention(config)\n",
        "        self.ln3 = nn.LayerNorm(config.n_embed)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, encoder_out):\n",
        "        b, t, c = x.size()\n",
        "        causal_mask = torch.tril(torch.ones(t, t)).view(1, 1, t, t).to(x.device)\n",
        "        x = x + self.mask_attn(self.ln1(x), causal_mask)\n",
        "        x = x + self.cross_attn(self.ln2(x), kv=encoder_out)\n",
        "        x = x + self.ffwd(self.ln3(x))\n",
        "        return x\n",
        "\n",
        "class WhisperEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(config.n_mels, config.n_embed, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(config.n_embed, config.n_embed, kernel_size=3, padding=1)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.n_audio_ctx, config.n_embed))\n",
        "        self.blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_audio_layers)])\n",
        "        self.ln_final = nn.LayerNorm(config.n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.size(1) != self.conv1.in_channels:\n",
        "            x = x.transpose(1, 2)\n",
        "        x = F.gelu(self.conv1(x))\n",
        "        x = F.gelu(self.conv2(x))\n",
        "        x = x.transpose(1, 2)\n",
        "        seq_len = x.size(1)\n",
        "        pos_emb = self.pos_embed[:, :seq_len, :]\n",
        "        x = x + pos_emb\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return self.ln_final(x)\n",
        "\n",
        "class WhisperDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.n_ctx, config.n_embed))\n",
        "        self.blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_text_layers)])\n",
        "        self.ln_final = nn.LayerNorm(config.n_embed)\n",
        "        self.head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
        "        self.token_embed.weight = self.head.weight\n",
        "\n",
        "    def forward(self, x, encoder_out):\n",
        "        b, t = x.size()\n",
        "        token_emb = self.token_embed(x)\n",
        "        pos_emb = self.pos_embed[:, :t, :]\n",
        "        x = token_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "            x = block(x, encoder_out)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb9vIOU0zfnV"
      },
      "outputs": [],
      "source": [
        "class WhisperModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = WhisperEncoder(config)\n",
        "        self.decoder = WhisperDecoder(config)\n",
        "\n",
        "    def forward(self, audio_features, decoder_input_ids):\n",
        "        encoder_out = self.encoder(audio_features)\n",
        "        logits = self.decoder(decoder_input_ids, encoder_out)\n",
        "        return logits\n",
        "\n",
        "class AudioProcessor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.sample_rate = config.sample_rate\n",
        "        self.n_mels = config.n_mels\n",
        "\n",
        "    def extract_fbank(self, audio_path):\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "            if waveform.size(0) > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
        "            mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=self.sample_rate,\n",
        "                n_fft=400,\n",
        "                hop_length=160,\n",
        "                win_length=400,\n",
        "                n_mels=self.n_mels\n",
        "            )(waveform)\n",
        "            log_mel = torch.log(mel_spec + 1e-9)\n",
        "            mean = log_mel.mean()\n",
        "            std = log_mel.std()\n",
        "            log_mel = (log_mel - mean) / (std + 1e-9)\n",
        "            return log_mel.squeeze(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio file {audio_path}: {e}\")\n",
        "            return torch.zeros(self.n_mels, 400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r6MPdbbzjZh"
      },
      "outputs": [],
      "source": [
        "class CommonVoiceDataset(Dataset):\n",
        "    def __init__(self, tsv_file, audio_dir, tokenizer, processor, max_audio_len=1500):\n",
        "        self.df = pd.read_csv(tsv_file, sep='\\t', on_bad_lines='skip')\n",
        "        self.df = self.df[self.df['valid'] == 1].reset_index(drop=True)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_audio_len = max_audio_len\n",
        "        print(f\"Dataset loaded with {len(self.df)} valid entries\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            audio_path = os.path.join(self.audio_dir, row['path'])\n",
        "            text = row['sentence']\n",
        "            mel_features = self.processor.extract_fbank(audio_path)\n",
        "            encoded_text = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                        max_length=100, truncation=True)\n",
        "            input_ids = encoded_text.input_ids.squeeze(0)\n",
        "            decoder_input_ids = input_ids[:-1].clone()\n",
        "            labels = input_ids[1:].clone()\n",
        "            if mel_features.size(1) > self.max_audio_len:\n",
        "                mel_features = mel_features[:, :self.max_audio_len]\n",
        "            elif mel_features.size(1) < self.max_audio_len:\n",
        "                pad_len = self.max_audio_len - mel_features.size(1)\n",
        "                mel_features = F.pad(mel_features, (0, pad_len))\n",
        "            mel_features = mel_features.unsqueeze(0)\n",
        "            return {\n",
        "                \"audio_features\": mel_features,\n",
        "                \"decoder_input_ids\": decoder_input_ids,\n",
        "                \"labels\": labels,\n",
        "                \"text\": text,\n",
        "                \"audio_path\": audio_path\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {e}\")\n",
        "            return {\n",
        "                \"audio_features\": torch.zeros(1, self.processor.n_mels, self.max_audio_len),\n",
        "                \"decoder_input_ids\": torch.zeros(99, dtype=torch.long),\n",
        "                \"labels\": torch.zeros(99, dtype=torch.long),\n",
        "                \"text\": \"\",\n",
        "                \"audio_path\": \"\"\n",
        "            }\n",
        "\n",
        "def collate_batch(batch):\n",
        "    try:\n",
        "        audio_features = torch.cat([item[\"audio_features\"] for item in batch], dim=0)\n",
        "        decoder_input_ids = pad_sequence([item[\"decoder_input_ids\"] for item in batch],\n",
        "                                        batch_first=True, padding_value=0)\n",
        "        labels = pad_sequence([item[\"labels\"] for item in batch],\n",
        "                            batch_first=True, padding_value=-100)\n",
        "        texts = [item[\"text\"] for item in batch]\n",
        "        audio_paths = [item[\"audio_path\"] for item in batch]\n",
        "        return {\n",
        "            \"audio_features\": audio_features,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"labels\": labels,\n",
        "            \"texts\": texts,\n",
        "            \"audio_paths\": audio_paths\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in collate_batch: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return {\n",
        "            \"audio_features\": torch.zeros(len(batch), 80, 1500),\n",
        "            \"decoder_input_ids\": torch.zeros(len(batch), 99, dtype=torch.long),\n",
        "            \"labels\": torch.zeros(len(batch), 99, dtype=torch.long),\n",
        "            \"texts\": [\"\"] * len(batch),\n",
        "            \"audio_paths\": [\"\"] * len(batch)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-FoYPPfzmah"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device, scaler=None, gradient_accumulation_steps=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with tqdm(total=num_batches, desc=\"Training\") as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            try:\n",
        "                audio_features = batch[\"audio_features\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                \n",
        "                if scaler is not None:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        logits = model(audio_features, decoder_input_ids)\n",
        "                        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1)) / gradient_accumulation_steps\n",
        "\n",
        "                    \n",
        "                    scaler.scale(loss).backward()\n",
        "\n",
        "                    if (i + 1) % gradient_accumulation_steps == 0 or i == num_batches - 1:\n",
        "                        \n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                        \n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "                else:\n",
        "                    \n",
        "                    logits = model(audio_features, decoder_input_ids)\n",
        "                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1)) / gradient_accumulation_steps\n",
        "                    loss.backward()\n",
        "\n",
        "                    if (i + 1) % gradient_accumulation_steps == 0 or i == num_batches - 1:\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "                \n",
        "                del audio_features, decoder_input_ids, labels, logits, loss\n",
        "                pbar.update(1)\n",
        "\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                print(f\"CUDA OOM error for batch {i}, skipping...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTJcj84377J2"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AqU6l_ynRkg"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device, tokenizer):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            try:\n",
        "                audio_features = batch[\"audio_features\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                texts = batch[\"texts\"]\n",
        "\n",
        "                logits = model(audio_features, decoder_input_ids)\n",
        "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                \n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "                for pred, ref in zip(predictions, texts):\n",
        "                    pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "                    all_predictions.append(pred_text)\n",
        "                    all_references.append(ref)\n",
        "\n",
        "                del audio_features, decoder_input_ids, labels, logits\n",
        "            except Exception as e:\n",
        "                print(f\"Error in evaluation: {e}\")\n",
        "                continue\n",
        "\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        error_rate = wer(all_references, all_predictions)\n",
        "    except ImportError:\n",
        "        print(\"jiwer not installed, skipping WER calculation\")\n",
        "        error_rate = 0\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, error_rate, all_predictions, all_references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqMIiafvnSHt"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, val_dataloader, optimizer, device, tokenizer,\n",
        "          num_epochs=30, checkpoint_dir=\"checkpoints\", use_amp=True):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_amp and torch.cuda.is_available() else None\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_wers = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        \n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, device, scaler)\n",
        "        train_losses.append(train_loss)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        \n",
        "        val_loss, val_wer, predictions, references = evaluate(model, val_dataloader, device, tokenizer)\n",
        "        val_losses.append(val_loss)\n",
        "        val_wers.append(val_wer)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, WER: {val_wer:.4f}\")\n",
        "\n",
        "        \n",
        "        for i in range(min(3, len(predictions))):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Reference: {references[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_wer': val_wer\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(checkpoint_dir, f\"whisper_model_epoch_{epoch+1}.pt\"))\n",
        "\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(checkpoint, os.path.join(checkpoint_dir, \"whisper_model_best.pt\"))\n",
        "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    \n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Train Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(val_wers, label='WER')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Word Error Rate')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(checkpoint_dir, 'training_history.png'))\n",
        "        print(f\"Training history saved to {os.path.join(checkpoint_dir, 'training_history.png')}\")\n",
        "    except ImportError:\n",
        "        print(\"matplotlib not installed, skipping plot generation\")\n",
        "\n",
        "    return train_losses, val_losses, val_wers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_ns0xYVHWi8"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP-8JLjcIack"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def augment_audio_features(mel_features, augment_prob=0.5):\n",
        "    \n",
        "    if random.random() < augment_prob:\n",
        "        \n",
        "        noise = torch.randn_like(mel_features) * 0.005\n",
        "        mel_features = mel_features + noise\n",
        "\n",
        "    if random.random() < 0.3:\n",
        "        \n",
        "        gain = random.uniform(0.8, 1.2)\n",
        "        mel_features = mel_features * gain\n",
        "\n",
        "    return mel_features\n",
        "\n",
        "\n",
        "\n",
        "class AugmentedCommonVoiceDataset(Dataset):\n",
        "    def __init__(self, tsv_file, audio_dir, tokenizer, processor, max_audio_len=1500, augment=False):\n",
        "        self.df = pd.read_csv(tsv_file, sep='\\\\t', on_bad_lines='skip')\n",
        "        self.df = self.df[self.df['valid'] == 1].reset_index(drop=True)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_audio_len = max_audio_len\n",
        "        self.augment = augment\n",
        "        print(f\"Dataset loaded with {len(self.df)} valid entries (augment={augment})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            audio_path = os.path.join(self.audio_dir, row['path'])\n",
        "            text = row['sentence']\n",
        "\n",
        "            mel_features = self.processor.extract_fbank(audio_path)\n",
        "\n",
        "            \n",
        "            if self.augment:\n",
        "                mel_features = augment_audio_features(mel_features, augment_prob=0.5)\n",
        "\n",
        "            encoded_text = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                        max_length=100, truncation=True)\n",
        "            input_ids = encoded_text.input_ids.squeeze(0)\n",
        "            decoder_input_ids = input_ids[:-1].clone()\n",
        "            labels = input_ids[1:].clone()\n",
        "\n",
        "            if mel_features.size(1) > self.max_audio_len:\n",
        "                mel_features = mel_features[:, :self.max_audio_len]\n",
        "            elif mel_features.size(1) < self.max_audio_len:\n",
        "                pad_len = self.max_audio_len - mel_features.size(1)\n",
        "                mel_features = F.pad(mel_features, (0, pad_len))\n",
        "\n",
        "            mel_features = mel_features.unsqueeze(0)\n",
        "\n",
        "            return {\n",
        "                \"audio_features\": mel_features,\n",
        "                \"decoder_input_ids\": decoder_input_ids,\n",
        "                \"labels\": labels,\n",
        "                \"text\": text,\n",
        "                \"audio_path\": audio_path\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {e}\")\n",
        "            return {\n",
        "                \"audio_features\": torch.zeros(1, self.processor.n_mels, self.max_audio_len),\n",
        "                \"decoder_input_ids\": torch.zeros(99, dtype=torch.long),\n",
        "                \"labels\": torch.zeros(99, dtype=torch.long),\n",
        "                \"text\": \"\",\n",
        "                \"audio_path\": \"\"\n",
        "            }\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, smoothing=0.1, ignore_index=-100):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        vocab_size = logits.size(-1)\n",
        "        logits_flat = logits.view(-1, vocab_size)\n",
        "        targets_flat = targets.view(-1)\n",
        "\n",
        "        mask = (targets_flat != self.ignore_index)\n",
        "        log_probs = F.log_softmax(logits_flat, dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / (vocab_size - 1))\n",
        "            true_dist.scatter_(1, targets_flat.unsqueeze(1), 1.0 - self.smoothing)\n",
        "            true_dist[~mask] = 0.0\n",
        "\n",
        "        loss = (-true_dist * log_probs).sum(dim=-1)\n",
        "        return loss[mask].mean()\n",
        "\n",
        "\n",
        "def train_epoch_improved(model, dataloader, optimizer, scheduler, device,\n",
        "                        scaler=None, gradient_accumulation_steps=4,\n",
        "                        label_smoothing=0.1):\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    criterion = LabelSmoothingLoss(smoothing=label_smoothing)\n",
        "\n",
        "    with tqdm(total=num_batches, desc=\"Training\") as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            try:\n",
        "                audio_features = batch[\"audio_features\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                if scaler is not None:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        logits = model(audio_features, decoder_input_ids)\n",
        "                        loss = criterion(logits, labels) / gradient_accumulation_steps\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "\n",
        "                    if (i + 1) % gradient_accumulation_steps == 0 or i == num_batches - 1:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "                else:\n",
        "                    logits = model(audio_features, decoder_input_ids)\n",
        "                    loss = criterion(logits, labels) / gradient_accumulation_steps\n",
        "                    loss.backward()\n",
        "\n",
        "                    if (i + 1) % gradient_accumulation_steps == 0 or i == num_batches - 1:\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "                \n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({'loss': f'{loss.item()*gradient_accumulation_steps:.4f}',\n",
        "                                'lr': f'{current_lr:.6f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "                del audio_features, decoder_input_ids, labels, logits, loss\n",
        "\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                print(f\"CUDA OOM error for batch {i}, skipping...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d0c737e"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \n",
        "    config = WhisperConfig(\n",
        "        n_mels=80,\n",
        "        n_ctx=1500,\n",
        "        n_heads=8,\n",
        "        n_audio_layers=4,\n",
        "        n_text_layers=4,\n",
        "        n_embed=512,\n",
        "        n_audio_ctx=1500,\n",
        "        vocab_size=51865,\n",
        "        sample_rate=16000,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\")\n",
        "    processor = AudioProcessor(config)\n",
        "\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
        "\n",
        "    \n",
        "    train_tsv = \"/content/drive/MyDrive/whisper/splits/train.tsv\"\n",
        "    val_tsv = \"/content/drive/MyDrive/whisper/splits/valid.tsv\"\n",
        "    audio_dir = \"/content/drive/MyDrive/whisper/cv-corpus-21.0-2025-03-14/mn/clips\"\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/whisper/whisper_checkpoints/\"\n",
        "\n",
        "    \n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = AugmentedCommonVoiceDataset(\n",
        "        train_tsv, audio_dir, tokenizer, processor, augment=True\n",
        "    )\n",
        "    val_dataset = AugmentedCommonVoiceDataset(\n",
        "        val_tsv, audio_dir, tokenizer, processor, augment=False\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, batch_size=4, shuffle=True,\n",
        "        collate_fn=collate_batch, num_workers=2, pin_memory=True\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset, batch_size=4, shuffle=False,\n",
        "        collate_fn=collate_batch, num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    \n",
        "    model = WhisperModel(config).to(device)\n",
        "\n",
        "    \n",
        "    checkpoint_path = \"/content/drive/MyDrive/whisper/whisper_checkpoints/whisper_model_epoch_24.pt\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"ERROR: Checkpoint not found at {checkpoint_path}\")\n",
        "        print(\"Available checkpoints:\")\n",
        "        for f in os.listdir(checkpoint_dir):\n",
        "            if f.endswith('.pt'):\n",
        "                print(f\"  - {f}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading checkpoint from epoch 24: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    start_epoch = 24\n",
        "    print(f\"Successfully loaded checkpoint from epoch {start_epoch}\")\n",
        "    print(f\"Previous WER: {checkpoint.get('val_wer', 'N/A')}\")\n",
        "\n",
        "    \n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=1e-5,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\\\nModel Statistics:\")\n",
        "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"  Initial learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    \n",
        "    num_epochs = 40\n",
        "    patience = 8\n",
        "    best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "    best_val_wer = checkpoint.get('val_wer', float('inf'))\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_wers = []\n",
        "\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\"Starting IMPROVED training from epoch {start_epoch+1}\")\n",
        "    print(f\"Improvements: Lower LR, Label Smoothing, Augmentation, LR Scheduler\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    \n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f\"\\\\n{'='*60}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        \n",
        "        train_loss = train_epoch_improved(\n",
        "            model, train_dataloader, optimizer, scheduler, device,\n",
        "            scaler=scaler, gradient_accumulation_steps=4,\n",
        "            label_smoothing=0.1\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        \n",
        "        val_loss, val_wer, predictions, references = evaluate(\n",
        "            model, val_dataloader, device, tokenizer\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "        val_wers.append(val_wer)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, WER: {val_wer:.4f}\")\n",
        "\n",
        "        \n",
        "        scheduler.step(val_wer)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        \n",
        "        print(\"\\\\nSample Predictions:\")\n",
        "        for i in range(min(3, len(predictions))):\n",
        "            print(f\"\\\\n  Example {i+1}:\")\n",
        "            print(f\"    Reference:  {references[i]}\")\n",
        "            print(f\"    Prediction: {predictions[i]}\")\n",
        "\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_wer': val_wer,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(checkpoint_dir, f\"whisper_model_epoch_{epoch+1}.pt\"))\n",
        "\n",
        "        \n",
        "        improved = False\n",
        "\n",
        "        if val_wer < best_val_wer:\n",
        "            improvement = ((best_val_wer - val_wer) / best_val_wer) * 100\n",
        "            best_val_wer = val_wer\n",
        "            torch.save(checkpoint, os.path.join(checkpoint_dir, \"whisper_model_best_wer.pt\"))\n",
        "            print(f\"\\\\n✓ NEW BEST WER! {val_wer:.4f} (improved by {improvement:.2f}%)\")\n",
        "            improved = True\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(checkpoint, os.path.join(checkpoint_dir, \"whisper_model_best_loss.pt\"))\n",
        "            print(f\"✓ New best loss! {val_loss:.4f}\")\n",
        "            improved = True\n",
        "\n",
        "        if improved:\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"\\\\nNo improvement ({patience_counter}/{patience} patience)\")\n",
        "\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\\\n⚠ Early stopping triggered after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "    \n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        \n",
        "        epochs_range = range(start_epoch+1, start_epoch+1+len(train_losses))\n",
        "\n",
        "        axes[0].plot(epochs_range, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "        axes[0].plot(epochs_range, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "        axes[0].axvline(x=start_epoch, color='g', linestyle='--', label='Resume Point')\n",
        "        axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[0].set_ylabel('Loss', fontsize=12)\n",
        "        axes[0].set_title('Training and Validation Loss (Improved)', fontsize=14)\n",
        "        axes[0].legend(fontsize=10)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1].plot(epochs_range, val_wers, 'g-', linewidth=2)\n",
        "        axes[1].axvline(x=start_epoch, color='g', linestyle='--', label='Resume Point')\n",
        "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[1].set_ylabel('Word Error Rate', fontsize=12)\n",
        "        axes[1].set_title('Validation WER (Improved)', fontsize=14)\n",
        "        axes[1].legend(fontsize=10)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(checkpoint_dir, 'training_history_improved.png')\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\\\n✓ Training history saved to {plot_path}\")\n",
        "        plt.close()\n",
        "    except ImportError:\n",
        "        print(\"\\\\n⚠ matplotlib not installed, skipping plot\")\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETED!\")\n",
        "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best WER: {best_val_wer:.4f}\")\n",
        "    print(f\"Starting WER (epoch 16): {checkpoint.get('val_wer', 'N/A')}\")\n",
        "    print(f\"Improvement: {((checkpoint.get('val_wer', best_val_wer) - best_val_wer) / checkpoint.get('val_wer', 1)) * 100:.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
